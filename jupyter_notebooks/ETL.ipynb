{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "0aStgWSO0E0E"
   },
   "source": [
    "UK MONETARY POLICY 2019-2025"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "1eLEkw5O0ECa"
   },
   "source": [
    "## Objectives\n",
    "\n",
    "* The objectives of this project are to:\n",
    "\n",
    "Collect official UK macroeconomic data from reliable public sources\n",
    "\n",
    "Apply Python-based ETL (Extract, Transform, Load) techniques\n",
    "\n",
    "Clean and preprocess time-series data for analysis\n",
    "\n",
    "Convert datasets to a common quarterly frequency for alignment\n",
    "\n",
    "Analyse trends in inflation, Official Bank Rate and GDP between 2019 and 2025\n",
    "## Inputs\n",
    "\n",
    "* The following data inputs required: \n",
    "1. Bank of England Policy Interest Rate : time series data on the UK nominal policy interest rate \n",
    "2. Consumer Price Index(CPI): UK CPI, used as a measure inflation\n",
    "3. Gross Domestic Product(GDP) : UK GDP data, used to represent economic activity. GDP\n",
    "\n",
    "## Outputs\n",
    "\n",
    "* Write here which files, code or artefacts you generate by the end of the notebook \n",
    "\n",
    "## Additional Comments\n",
    "\n",
    "* If you have any additional comments that don't fit in the previous bullets, please state them here. \n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "9uWZXH9LwoQg"
   },
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "cqP-UeN-z3i2"
   },
   "source": [
    "# Change working directory"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* We are assuming you will store the notebooks in a subfolder, therefore when running the notebook in the editor, you will need to change the working directory"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "aOGIGS-uz3i2"
   },
   "source": [
    "We need to change the working directory from its current folder to its parent folder\n",
    "* We access the current directory with os.getcwd()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "wZfF_j-Bz3i4",
    "outputId": "66943449-1436-4c3d-85c7-b85f9f78349b"
   },
   "outputs": [],
   "source": [
    "import os\n",
    "current_dir = os.getcwd() #\n",
    "current_dir"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "9MWW8E7lz3i7"
   },
   "source": [
    "We want to make the parent of the current directory the new current directory\n",
    "* os.path.dirname() gets the parent directory\n",
    "* os.chir() defines the new current directory"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "TwHsQRWjz3i9",
    "outputId": "86849db3-cd2f-4cc5-ebb8-2d0caafa1a2c"
   },
   "outputs": [],
   "source": [
    "os.chdir(os.path.dirname(current_dir))\n",
    "print(\"You set a new current directory\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "M_xPk_Ijz3i-"
   },
   "source": [
    "Confirm the new current directory"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "vz3S-_kjz3jA",
    "outputId": "00b79ae4-75d0-4a96-d193-ac9ef9847ea2"
   },
   "outputs": [],
   "source": [
    "current_dir = os.getcwd()\n",
    "current_dir"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "-mavJ8DibrcQ"
   },
   "source": [
    "# Section 1\n",
    "#Import Libraries\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Section 1 Extraction: Load the Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "\n",
    "cpi = pd.read_csv(r\"\\\\talktalk\\redirectedfolders\\F.Afolabi\\Documents\\VSCode1\\inflation.csv\")\n",
    "print(cpi.head())\n",
    "\n",
    "# Display basic information about the dataset, I would take the variable one by one; manipulating CPI first; \n",
    "cpi.info() \n",
    "\n",
    "print(cpi.shape)  # Looking at the shape of CPI"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Generate a summary of the statistics\n",
    "print(cpi.describe())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Checking for missing values and data type\n",
    "print(cpi.isnull().sum())\n",
    "print(cpi.dtypes)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Converting Date to datetime\n",
    "cpi['Date'] = pd.to_datetime(cpi['Date'], errors='coerce')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Filtering the data between 2019 and 2025 for CPI for the analysis and the scope of this project\n",
    "cpi = cpi[(cpi['Date'] >= '2019-01-01') & (cpi['Date'] <= '2025-12-31')]\n",
    "print(cpi.head())\n",
    "cpi.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#filling missing values using interpolation method\n",
    "cpi['CPI_Inflation'] = cpi['CPI_Inflation'].interpolate()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Converting CPI From Annual to Quarterly Frequency"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Annual aggregation of CPI Inflation\n",
    "cpi['Year'] = cpi['Date'].dt.year\n",
    "cpi_annual = (cpi.groupby('Year', as_index=False).agg({'CPI_Inflation': 'mean'}))\n",
    "print(cpi_annual)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Converting annual CPI to quarterly CPI by forward filling the values\n",
    "\n",
    "q_index = pd.period_range('2019Q1', '2025Q4', freq='Q')\n",
    "\n",
    "cpi_q = cpi_annual.copy()\n",
    "\n",
    "# Creating a  Date column from Year\n",
    "cpi_q['Date'] = pd.PeriodIndex(cpi_q['Year'].astype(str) + 'Q1', freq='Q')\n",
    "\n",
    "cpi_q = cpi_q[['Date', 'CPI_Inflation']] # Selecting only the necessary columns\n",
    "\n",
    "\n",
    "\n",
    "#Reindexing and forward filling\n",
    "cpi_q = (cpi_q.set_index('Date').reindex(q_index).ffill().reset_index().rename(columns={'index': 'Date'}))\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(len(cpi_q))\n",
    "print(cpi_q.head())\n",
    "print(cpi_q.tail())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cpi_q.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    " cpi_q_cleaned = cpi_q.to_csv('cpi_q_cleaned.csv', index=False) # Saving the cleaned CPI data to a new CSV file, for further analysis"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Loading the Official Bank Rate to be cleaned. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "boe_rate = pd.read_csv(r\"\\\\talktalk\\redirectedfolders\\F.Afolabi\\Documents\\VSCode1\\Bank_rate.csv\")\n",
    "print(boe_rate.head())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#boe_rate = pd.read_csv('../Dataset/Raw/Bank_rate.csv')\n",
    "#boe_rate.head()\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "boe_rate.info() #Looking at the data types and missing values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "vscode": {
     "languageId": "markdown"
    }
   },
   "outputs": [],
   "source": [
    "# Rename the Colum, I rename the column to be consistent across the dataset\n",
    "\n",
    "boe_rate.columns = ['Date', 'Bank_Rate']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "boe_rate.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(boe_rate.columns)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Convert Date column safely (UK date format)\n",
    "boe_rate['Date'] = pd.to_datetime(\n",
    "    boe_rate['Date'],\n",
    "    errors='coerce',\n",
    "    dayfirst=True\n",
    ")\n",
    "\n",
    "# Remove rows with invalid dates\n",
    "boe_rate = boe_rate.dropna(subset=['Date'])\n",
    "\n",
    "# Set Date as index for resampling\n",
    "boe_rate = boe_rate.set_index('Date')\n",
    "\n",
    "# Resample to quarterly frequency (mean Bank Rate)\n",
    "boe_rate_q = boe_rate[['Bank_Rate']].resample('QS').mean()\n",
    "\n",
    "print(boe_rate_q.head())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Filtering the data between 2019 and 2025 for Bank Rate  for the analysis and the scope of this project, date reset to allow filtering\n",
    "boe_rate.reset_index(inplace=True)\n",
    "boe_rate_filtered = boe_rate[(boe_rate['Date'] >= '2019-01-01') & (boe_rate['Date'] <= '2025-12-31')]\n",
    "print(boe_rate_filtered.head())\n",
    "boe_rate_filtered.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "boe_rate_filtered.dtypes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "boe_rate_filtered.info()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Checking for missing values\n",
    "print(boe_rate_filtered.isnull().sum())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Checking for missing Quaterly periods\n",
    "#boe_rate_filtered = boe_rate_filtered.asfreq('QS')\n",
    "#print(boe_rate_filtered.isnull().sum())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#print Bank Rtae info\n",
    "boe_rate_filtered.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Resetting index\n",
    "\n",
    "# boe_rate_filtered=boe_rate_filtered.reset_index() "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Dropping the index column because it is not needed\n",
    "#boe_rate_filtered = boe_rate_filtered.drop(columns=['index'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Convert Date to Period for consistency\n",
    "#boe_rate_filtered['Date'] = boe_rate_filtered['Date'].dt.to_period('Q')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(boe_rate_filtered['Date'].dtype)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# saved the cleaned Bank Rate data to a new CSV file, for further analysis\n",
    "boe_rate_cleaned = boe_rate_filtered.to_csv('boe_rate_cleaned.csv', index=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(boe_rate_filtered.head())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Loading GDP data\n",
    "gdp = pd.read_csv('../Dataset/Raw/GDP.csv')\n",
    "gdp.head(5)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(gdp.info)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(gdp.columns)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Finding all columns containing 'Gross' or 'GDP' becasue they are relevant to our analysis \n",
    "gdp_columns = [col for col in gdp.columns if 'Gross' in col or 'GDP' in col]\n",
    "print(gdp_columns)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for col in gdp.columns:\n",
    "    print(repr(col))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "gdp.columns = (\n",
    "    gdp.columns\n",
    "      .astype(str)\n",
    "      .str.encode('latin1', errors='ignore').str.decode('utf-8', errors='ignore')\n",
    "      .str.replace(r'\\s+', ' ', regex=True)\n",
    "      .str.replace('\\\\', '', regex=False)\n",
    "      .str.strip()\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(list(gdp.columns))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for i, col in enumerate(gdp.columns):\n",
    "    print(i, repr(col))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# SLICING: I need to create a new dataframe with only the relevant columns\n",
    "\n",
    "gdp_clean = gdp[['Date', 'Gross Domestic Product']].copy() \n",
    "\n",
    "# Rename columns for clarity\n",
    "gdp_clean = gdp_clean.rename(columns={'Gross Domestic Product': 'GDP_Level'})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "gdp_clean.head()\n",
    "gdp_clean.info()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Converting 'Date' to string to enable filtering\n",
    "\n",
    "\n",
    "\n",
    "#converting GDP to numeric\n",
    "gdp_clean['GDP_Level'] = pd.to_numeric(gdp_clean['GDP_Level'], errors='coerce')\n",
    "\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# filtering for the period 2019 Q1 to 2025 Q4\n",
    "gdp_clean = gdp_clean[(gdp_clean['Date'] >= '2019Q1') & (gdp_clean['Date'] <= '2025Q4')].copy()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "gdp_clean.info()\n",
    "gdp_clean.head()\n",
    "gdp_clean.tail()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(gdp_clean['Date'].min(), gdp_clean['Date'].max())\n",
    "print(gdp_clean['Date'].dtype)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "GDP dates were converted to quarterly period format for alignment with both Bank rate and Inflation(CPI)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "gdp_clean_cleaned = gdp_clean.to_csv('gdp_cleaned.csv', index=False) # Saving the cleaned GDP data to a new CSV file, for further analysis\n",
    "gdp_clean_cleaned"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(gdp_clean['Date'].dtype)\n",
    "print(boe_rate_filtered['Date'].dtype)\n",
    "print(cpi_q['Date'].dtype)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "final_df = (\n",
    "    gdp_clean\n",
    "    .merge(boe_rate_filtered, on='Date', how='left')\n",
    "    .merge(cpi_q, on='Date', how='left')\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(final_df.head())\n",
    "print(final_df.info())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"GDP dates:\")\n",
    "print(gdp_clean['Date'].min(), gdp_clean['Date'].max())\n",
    "\n",
    "print(\"Bank Rate dates:\")\n",
    "print(boe_rate_filtered['Date'].min(), boe_rate_filtered['Date'].max())\n",
    "\n",
    "print(\"CPI dates:\")\n",
    "print(cpi_q['Date'].min(), cpi_q['Date'].max())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# save the final df to csv file\n",
    "final_df.to_csv(\"final_df\", index=False)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "ZY3l0-AxO93d"
   },
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "uFQo3ycuO-v6"
   },
   "source": [
    "# Section 2"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Section 2 content"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "NOTE"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* You may add as many sections as you want, as long as it supports your project workflow.\n",
    "* All notebook's cells should be run top-down (you can't create a dynamic wherein a given point you need to go back to a previous cell to execute some task, like go back to a previous cell and refresh a variable content)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "ltNetd085qHf"
   },
   "source": [
    "# Push files to Repo"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* In cases where you don't need to push files to Repo, you may replace this section with \"Conclusions and Next Steps\" and state your conclusions and next steps."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "aKlnIozA4eQO",
    "outputId": "fd09bc1f-adb1-4511-f6ce-492a6af570c0"
   },
   "outputs": [],
   "source": [
    "import os\n",
    "#try:\n",
    "  # create your folder here\n",
    "  # os.makedirs(name='')\n",
    "#except Exception as e:\n",
    " # print(e)\n"
   ]
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "name": "Data Practitioner Jupyter Notebook.ipynb",
   "provenance": [],
   "toc_visible": true
  },
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.8"
  },
  "orig_nbformat": 2
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
